{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8503864,
          "sourceType": "datasetVersion",
          "datasetId": 5075450
        }
      ],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step1: Install amd import libraries**"
      ],
      "metadata": {
        "id": "p-u48Mx3_SmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq \"transformers==4.35\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" \"tiktoken\"\n",
        "!pip install -qq gradio\n",
        "!pip install -qq gtts\n",
        "\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install datasets\n",
        "\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from random import randrange\n",
        "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "import transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "print(transformers.__version__)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device ',device,' is being used')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xmga9ZAr_Sme",
        "outputId": "30f6a2f8-1f2a-4996-9840-906e72c4a0ec",
        "execution": {
          "iopub.status.busy": "2024-05-24T10:40:17.316857Z",
          "iopub.execute_input": "2024-05-24T10:40:17.317178Z",
          "iopub.status.idle": "2024-05-24T10:40:50.756762Z",
          "shell.execute_reply.started": "2024-05-24T10:40:17.317155Z",
          "shell.execute_reply": "2024-05-24T10:40:50.755783Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.9/315.9 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.3/401.3 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.23.1 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:533: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
            "  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'top_k_top_p_filtering' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f0069a4dec3b>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoPeftModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoPeftModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.4.7\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBestOfNSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_peft_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtop_k_top_p_filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'top_k_top_p_filtering' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Load Saved Model & Questions Embedding**"
      ],
      "metadata": {
        "id": "yuCnbAxj_Smt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 15GwtRsgdPNkgQ-15WLsmZa6Dsd-hijmH\n",
        "\n",
        "!gdown 1hoQoV8daHWtsYZ5znxXLg1OpP0-CF60E"
      ],
      "metadata": {
        "id": "4bRfbBLzV1YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2-1 Load tokenizer"
      ],
      "metadata": {
        "id": "VvLN--N5_Smv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "epochs = 2\n",
        "batch_size = 1\n",
        "learning_rate = 5e-5\n",
        "adam_epsilon = 1e-8\n",
        "warmup_steps = 0\n",
        "train_questions = 91772\n",
        "\n",
        "\n",
        "# Load the T5 tokenizer and model\n",
        "model_name = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
        "total_steps = (train_questions) // batch_size * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
        "    checkpoint = torch.load(checkpoint_path,  map_location=torch.device(device))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1  # start from the next epoch\n",
        "    loss = checkpoint['loss']\n",
        "    return model, optimizer, scheduler, start_epoch, loss\n",
        "\n",
        "# Initialize variables\n",
        "start_epoch = 0  # default start epoch\n",
        "checkpoint_path = '/content/checkpoint_epoch_1_batch_40000.pt'\n",
        "\n",
        "model, optimizer, scheduler, start_epoch, loss = load_checkpoint(checkpoint_path, model, optimizer, scheduler)\n",
        "print(f\"Loaded checkpoint from epoch {start_epoch}, loss: {loss}\")"
      ],
      "metadata": {
        "id": "QxezgUaBy1Eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Load Dataset for creating contexts using questions embedding**"
      ],
      "metadata": {
        "id": "sTyCkSg_yedD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data\n",
        "def organize_dataset(dataset):\n",
        "  questions = dataset['title']\n",
        "  questions_selftext = dataset['selftext']\n",
        "  categories = dataset['category']\n",
        "\n",
        "  answers = []\n",
        "  answers_scores = []\n",
        "  for item in dataset:\n",
        "    answers.append(item['answers']['text'])\n",
        "    answers_scores.append(item['answers']['score'])\n",
        "\n",
        "  dataset_dict = {\"questions\":questions, \"selftext\": questions_selftext, \"category\":categories, \"answers\":answers, \"score\":answers_scores}\n",
        "  return dataset_dict\n",
        "\n",
        "dataset = load_dataset(\"eli5_category\")\n",
        "\n",
        "train_dataset_dict = organize_dataset(dataset['train'])\n",
        "valid1_dataset_dict = organize_dataset(dataset['validation1'])\n",
        "valid2_dataset_dict = organize_dataset(dataset['validation2'])\n",
        "test_dataset_dict = organize_dataset(dataset['test'])\n",
        "\n",
        "train_questions = train_dataset_dict['questions']\n",
        "train_answers = train_dataset_dict['answers']\n",
        "train_scores = train_dataset_dict['score']\n",
        "\n",
        "valid_questions = valid1_dataset_dict['questions']\n",
        "valid_answers = valid1_dataset_dict['answers']\n",
        "valid_scores = valid1_dataset_dict['score']\n",
        "\n",
        "all_questions = train_dataset_dict['questions'] + valid1_dataset_dict['questions'] + valid2_dataset_dict['questions']\n",
        "all_answers = train_dataset_dict['answers'] + valid1_dataset_dict['answers'] + valid2_dataset_dict['answers']\n",
        "all_scores = train_dataset_dict['score'] + valid1_dataset_dict['score'] + valid2_dataset_dict['score']\n",
        "\n",
        "\n",
        "# Load models\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "rbQEx3a3yTei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define weighted sampling function\n",
        "def weighted_sample(answers, scores, k=3):\n",
        "    k = min(k, len(answers))\n",
        "\n",
        "    if len(answers) == 0:\n",
        "        return []\n",
        "\n",
        "    total_score = sum(scores)\n",
        "    probabilities = [score / total_score for score in scores]\n",
        "    sampled_indices = np.random.choice(len(answers), size=k, replace=False, p=probabilities)\n",
        "    return [answers[i] for i in sampled_indices]\n",
        "\n",
        "def index_Q_embeddings(question_embeddings):\n",
        "    # Index embeddings using FAISS\n",
        "    index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
        "    index.add(question_embeddings.cpu().numpy())\n",
        "    return index\n",
        "\n",
        "def load_Q_embeddings(train_question_embeddings_file='/kaggle/working/question_embeddings.npy'):\n",
        "    # Load the embeddings from the file\n",
        "    train_questions_embeddings = torch.tensor(np.load(train_question_embeddings_file))\n",
        "    print(\"Loaded Question Embeddings.\")\n",
        "    return train_questions_embeddings\n",
        "\n",
        "def find_relevant_context_SBERT(question, question_embeddings, answers, scores, index, use='train'):\n",
        "    '''\n",
        "    this function returns top-2 similar answers for training phase\n",
        "    - top-2 answers of similar questions\n",
        "    - top-2 answers of current question\n",
        "    '''\n",
        "    new_question_embedding = sbert_model.encode(question, convert_to_tensor=True, show_progress_bar=False).unsqueeze(0)\n",
        "    new_question_embedding = torch.nn.functional.normalize(new_question_embedding, p=2, dim=1)\n",
        "\n",
        "    k = 5  # Retrieve a larger number of candidates\n",
        "    D, I = index.search(new_question_embedding.cpu().numpy(), k)\n",
        "\n",
        "    threshold = 0.5\n",
        "    filtered_indices = []\n",
        "    for i, distance in zip(I[0], D[0]):\n",
        "        candidate_embedding = question_embeddings[i].cpu().numpy().reshape(1, -1)\n",
        "        similarity = cosine_similarity(new_question_embedding.cpu().numpy(), candidate_embedding)[0][0]\n",
        "        if similarity >= threshold:\n",
        "            filtered_indices.append(i)\n",
        "\n",
        "    if (len(filtered_indices) == 0):\n",
        "        if(use == 'test'):\n",
        "            return [\"\"], [\"\"]\n",
        "        else:\n",
        "            n_index = all_questions.index(question)\n",
        "            top_ans = weighted_sample(answers[n_index], scores[n_index], k=3)\n",
        "            return top_ans, [\"\"]\n",
        "    else:\n",
        "        # Retrieve relevant contexts (questions and answers)\n",
        "        relevant_contexts = [weighted_sample(answers[i], scores[i], k=3) for i in filtered_indices[1:]]\n",
        "        if len(relevant_contexts) >= 1:  # when no relevant context is found\n",
        "            relevant_contexts = relevant_contexts[0]\n",
        "        top_2_ans = weighted_sample(answers[filtered_indices[0]], scores[filtered_indices[0]], k=3)\n",
        "        return top_2_ans, relevant_contexts\n",
        "\n",
        "# Load questions embedding\n",
        "question_embeddings = load_Q_embeddings(\"/content/question_embeddings.npy\")\n",
        "faiss_index = index_Q_embeddings(question_embeddings)"
      ],
      "metadata": {
        "id": "eoO3UwfQytM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Generate answer**"
      ],
      "metadata": {
        "id": "qV3TBOvq_Smy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_query(model, tokenizer, query,temperature=0.6,long_answer = False):\n",
        "\n",
        "    context = find_relevant_context_SBERT(query, question_embeddings, all_answers, all_scores, faiss_index, use='test')\n",
        "    prompt = \"question: {} context: {}\".format(query, ' '.join(context[0] + context[1]))\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "\n",
        "    if long_answer:\n",
        "\n",
        "        outputs = model.generate(input_ids=input_ids,\n",
        "\n",
        "\n",
        "                             min_length=100,\n",
        "                             max_new_tokens=350,\n",
        "                                 #repetition_penalty=0.2,\n",
        "                            #  do_sample=True,\n",
        "                            #  top_p=0.9,\n",
        "                             temperature=temperature)\n",
        "\n",
        "    else:\n",
        "\n",
        "        outputs = model.generate(input_ids=input_ids,\n",
        "\n",
        "\n",
        "                             min_length=40,\n",
        "                             #max_length = 100,\n",
        "                             max_new_tokens=100,\n",
        "                                 #repetition_penalty=0.2,\n",
        "                            #  do_sample=True,\n",
        "                            #  top_p=0.9,\n",
        "                             temperature=temperature)\n",
        "    answer = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
        "\n",
        "    return prompt,answer"
      ],
      "metadata": {
        "id": "DWpXtN9R_Smy",
        "execution": {
          "iopub.status.busy": "2024-05-24T10:44:57.689249Z",
          "iopub.execute_input": "2024-05-24T10:44:57.689548Z",
          "iopub.status.idle": "2024-05-24T10:44:57.696993Z",
          "shell.execute_reply.started": "2024-05-24T10:44:57.689523Z",
          "shell.execute_reply": "2024-05-24T10:44:57.696085Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Ueser Interface**"
      ],
      "metadata": {
        "id": "NRNv0U_CkM19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check availabilty:\n",
        "def disability(query,long_answer):\n",
        "  result = 0\n",
        "  while len(result)<20: # Generate again if, last time model was disable\n",
        "      answer = answer_query(query,long_answer=long_answer)\n",
        "      new_answer = pretty_asnwer(answer)\n",
        "      result = new_answer\n",
        "\n",
        "  return result\n",
        "\n",
        "def text_to_speech(text):\n",
        "        # Convert text to speech\n",
        "        tts = gTTS(text)\n",
        "        # Save the converted audio to a file\n",
        "        tts.save(\"output.mp3\")\n",
        "        # Return the file path\n",
        "        return \"output.mp3\"\n",
        "\n",
        "def aks_question(Question, Long_Format, Audio_Play ):\n",
        "    query = Question\n",
        "\n",
        "    print(query)\n",
        "\n",
        "    #check Long format\n",
        "\n",
        "    if Long_Format == True:\n",
        "\n",
        "        prompt,long_answer = answer_query(model, tokenizer, query,temperature=0.6,long_answer = True)\n",
        "        # long_answer = pretty_asnwer(prompt,long_answer)\n",
        "\n",
        "        if Audio_Play:\n",
        "          text = 'Long answer:'+ long_answer\n",
        "          audio_ouput = text_to_speech(text)\n",
        "          return long_answer, audio_ouput\n",
        "\n",
        "        else:\n",
        "          text = ' You did not chose audio play'\n",
        "          audio_ouput = text_to_speech(text)\n",
        "          return long_answer, audio_ouput\n",
        "\n",
        "    else:\n",
        "        text = ' Please check one checkbox at least'\n",
        "        audio_ouput = text_to_speech(text)\n",
        "        return 'Please check one checkbox at least', audio_ouput\n",
        "\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=aks_question,\n",
        "\n",
        "    inputs=[\"text\", \"checkbox\",\"checkbox\"],\n",
        "\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Long Answer\"),\n",
        "        gr.Audio(label='Long_Format_Audio_Play')\n",
        "    ]\n",
        ")\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "k8SjKJU_ncUm",
        "outputId": "aabc25c9-8705-4957-ec03-4075ebe257b0"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://fb30f80508663e603f.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fb30f80508663e603f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KTEKQPvroQMl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}