{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8503864,
          "sourceType": "datasetVersion",
          "datasetId": 5075450
        }
      ],
      "dockerImageVersionId": 30698,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Step1: Install amd import libraries**"
      ],
      "metadata": {
        "id": "p-u48Mx3_SmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq \"transformers==4.35\" \"peft==0.4.0\" \"accelerate==0.21.0\" \"bitsandbytes==0.40.2\" \"trl==0.4.7\" \"safetensors>=0.3.1\" \"tiktoken\"\n",
        "!pip install -qq gradio\n",
        "!pip install -qq gtts\n",
        "import pandas as pd\n",
        "import gradio as gr\n",
        "from gtts import gTTS\n",
        "import os\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from random import randrange\n",
        "from peft import LoraConfig, get_peft_model, AutoPeftModelForCausalLM\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "from peft import AutoPeftModelForCausalLM\n",
        "import transformers\n",
        "\n",
        "print(transformers.__version__)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('Device ',device,' is being used')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xmga9ZAr_Sme",
        "outputId": "30f6a2f8-1f2a-4996-9840-906e72c4a0ec",
        "execution": {
          "iopub.status.busy": "2024-05-24T10:40:17.316857Z",
          "iopub.execute_input": "2024-05-24T10:40:17.317178Z",
          "iopub.status.idle": "2024-05-24T10:40:50.756762Z",
          "shell.execute_reply.started": "2024-05-24T10:40:17.317155Z",
          "shell.execute_reply": "2024-05-24T10:40:50.755783Z"
        },
        "trusted": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.9/315.9 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.3/401.3 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "tokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.23.1 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:533: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
            "  \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'top_k_top_p_filtering' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-f0069a4dec3b>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLoraConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoPeftModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBitsAndBytesConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFTTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpeft\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoPeftModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.4.7\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBestOfNSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mimport_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_peft_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/trl/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtop_k_top_p_filtering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'top_k_top_p_filtering' from 'transformers' (/usr/local/lib/python3.10/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Load Saved Model**"
      ],
      "metadata": {
        "id": "yuCnbAxj_Smt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model_3\n",
        "! gdown 17Z8n0X53ho-C_vcc1m2bKanf8pwjt_dw\n",
        "print(' If you cannot download dataset: please use this link:\\n https://drive.google.com/file/d/17Z8n0X53ho-C_vcc1m2bKanf8pwjt_dw/view?usp=sharing')\n"
      ],
      "metadata": {
        "id": "4bRfbBLzV1YG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq '/content/Final_Laama_Model.zip'"
      ],
      "metadata": {
        "id": "M8soIPSyyEtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_directory = '/content/results/lamma/checkpoint-2100' # Set this to your directory\n",
        "\n",
        "new_model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "    model_directory,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "metadata": {
        "id": "iUApKhNj_Smu",
        "execution": {
          "iopub.status.busy": "2024-05-24T10:42:02.571086Z",
          "iopub.execute_input": "2024-05-24T10:42:02.571461Z",
          "iopub.status.idle": "2024-05-24T10:44:57.539474Z",
          "shell.execute_reply.started": "2024-05-24T10:42:02.571430Z",
          "shell.execute_reply": "2024-05-24T10:44:57.538678Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2-1 Load tokenizer"
      ],
      "metadata": {
        "id": "VvLN--N5_Smv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_directory, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "sp_l2lEC_Smw",
        "execution": {
          "iopub.status.busy": "2024-05-24T10:44:57.541401Z",
          "iopub.execute_input": "2024-05-24T10:44:57.542063Z",
          "iopub.status.idle": "2024-05-24T10:44:57.688206Z",
          "shell.execute_reply.started": "2024-05-24T10:44:57.542023Z",
          "shell.execute_reply": "2024-05-24T10:44:57.687461Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Generate answer**"
      ],
      "metadata": {
        "id": "qV3TBOvq_Smy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_query(query,temperature=0.6,long_answer = False):\n",
        "\n",
        "    if long_answer:\n",
        "      prompt= 'Generatee long answer for this question ' + query\n",
        "\n",
        "    else:\n",
        "      prompt= 'Generate short answer for this question ' + query\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
        "\n",
        "    if long_answer:\n",
        "\n",
        "        outputs = new_model.generate(input_ids=input_ids,\n",
        "\n",
        "\n",
        "                             min_length=100,\n",
        "                             max_new_tokens=300,\n",
        "                                 #repetition_penalty=0.2,\n",
        "                            #  do_sample=True,\n",
        "                            #  top_p=0.9,\n",
        "                             temperature=temperature)\n",
        "\n",
        "    else:\n",
        "\n",
        "        outputs = new_model.generate(input_ids=input_ids,\n",
        "\n",
        "\n",
        "                             min_length=40,\n",
        "                             #max_length = 100,\n",
        "                             max_new_tokens=70,\n",
        "                                 #repetition_penalty=0.2,\n",
        "                            #  do_sample=True,\n",
        "                            #  top_p=0.9,\n",
        "                             temperature=temperature)\n",
        "    answer = tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True)[0]\n",
        "\n",
        "    return prompt,answer"
      ],
      "metadata": {
        "id": "DWpXtN9R_Smy",
        "execution": {
          "iopub.status.busy": "2024-05-24T10:44:57.689249Z",
          "iopub.execute_input": "2024-05-24T10:44:57.689548Z",
          "iopub.status.idle": "2024-05-24T10:44:57.696993Z",
          "shell.execute_reply.started": "2024-05-24T10:44:57.689523Z",
          "shell.execute_reply": "2024-05-24T10:44:57.696085Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-1 Make answer pretty"
      ],
      "metadata": {
        "id": "D0a6mDKskERG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove question from answer and remove last uncomplete sentence\n",
        "def pretty_asnwer(prompt,answer):\n",
        "    new_answer = answer\n",
        "    if new_answer.endswith('.') or new_answer.endswith('!') or new_answer.endswith('?'):\n",
        "\n",
        "        last_index_of_question = len(prompt)\n",
        "        new_answer = new_answer[last_index_of_question:]\n",
        "        if new_answer.find('Answer')>-1:\n",
        "          index = new_answer.find('Answer')\n",
        "          new_answer = new_answer[index+8:]\n",
        "        #print(new_answer)\n",
        "\n",
        "    else:\n",
        "        #print('Not complete answer')\n",
        "        last_index_of_question = len(prompt)\n",
        "        new_answer = new_answer[last_index_of_question:]\n",
        "        last_sentence = new_answer.split('. ')[-1]\n",
        "        new_answer = new_answer.replace(last_sentence, \" \")\n",
        "        if new_answer.find('Answer')>-1:\n",
        "          #print('here')\n",
        "          index = new_answer.find('Answer')\n",
        "          new_answer = new_answer[index+8:]\n",
        "\n",
        "\n",
        "\n",
        "    return new_answer\n",
        "\n"
      ],
      "metadata": {
        "id": "hUqmNdQo_Smz",
        "execution": {
          "iopub.status.busy": "2024-05-24T10:44:57.698942Z",
          "iopub.execute_input": "2024-05-24T10:44:57.699555Z",
          "iopub.status.idle": "2024-05-24T10:44:57.706591Z",
          "shell.execute_reply.started": "2024-05-24T10:44:57.699523Z",
          "shell.execute_reply": "2024-05-24T10:44:57.705822Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Ueser Interface**"
      ],
      "metadata": {
        "id": "NRNv0U_CkM19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check availabilty:\n",
        "def disability(query,long_answer):\n",
        "  result = 0\n",
        "  while len(result)<20: # Generate again if, last time model was disable\n",
        "      answer = answer_query(query,long_answer=long_answer)\n",
        "      new_answer = pretty_asnwer(answer)\n",
        "      result = new_answer\n",
        "\n",
        "  return result\n",
        "\n",
        "def text_to_speech(text):\n",
        "        # Convert text to speech\n",
        "        tts = gTTS(text)\n",
        "        # Save the converted audio to a file\n",
        "        tts.save(\"output.mp3\")\n",
        "        # Return the file path\n",
        "        return \"output.mp3\"\n",
        "\n",
        "def aks_question(Question, Long_Format, Short_Format, Audio_Play ):\n",
        "    query = Question\n",
        "\n",
        "    #check Long format\n",
        "\n",
        "    if Long_Format == True and Short_Format == True:\n",
        "        # query_copy_1 = query\n",
        "        # query_copy_2 = query\n",
        "\n",
        "        prompt,long_answer = answer_query(query,temperature=0.6,long_answer = True)\n",
        "        long_answer = pretty_asnwer(prompt,long_answer)\n",
        "\n",
        "        while len(long_answer)<50: # Generate again if, last time model was disable\n",
        "            prompt,long_answer = answer_query(query,temperature=0.6,long_answer = True)\n",
        "            long_answer = pretty_asnwer(long_answer)\n",
        "\n",
        "\n",
        "        prompt,short_answer = answer_query(query,temperature=0.6,long_answer = False)\n",
        "        short_answer = pretty_asnwer(prompt,short_answer)\n",
        "\n",
        "\n",
        "        while len(short_answer)<30: # Generate again if, last time model was disable\n",
        "          prompt,short_answer = answer_query(query,temperature=0.6,long_answer = False)\n",
        "          short_answer = pretty_asnwer(prompt,short_answer)\n",
        "\n",
        "        if Audio_Play:\n",
        "          text = 'Long answer:'+ long_answer + 'Short answer:'+short_answer\n",
        "          audio_ouput = text_to_speech(text)\n",
        "          return long_answer, short_answer, audio_ouput\n",
        "\n",
        "        else:\n",
        "          text = ' You did not chose audio play'\n",
        "          audio_ouput = text_to_speech(text)\n",
        "          return long_answer, short_answer, audio_ouput\n",
        "\n",
        "\n",
        "    elif Long_Format == True and Short_Format == False:\n",
        "        prompt,long_answer = answer_query(query,temperature=0.6,long_answer = True)\n",
        "        long_answer = pretty_asnwer(prompt,long_answer)\n",
        "\n",
        "        while len(long_answer)<50: # Generate again if, last time model was disable\n",
        "            prompt,long_answer = answer_query(query,temperature=0.6,long_answer = True)\n",
        "            long_answer = pretty_asnwer(long_answer)\n",
        "\n",
        "\n",
        "        if Audio_Play:\n",
        "          text = 'Long answer:'+ long_answer\n",
        "          audio_ouput = text_to_speech(text)\n",
        "          return long_answer, ' ', audio_ouput\n",
        "\n",
        "        else:\n",
        "          text = ' You did not chose audio play'\n",
        "          audio_ouput = text_to_speech(text)\n",
        "          return long_answer, ' ', audio_ouput\n",
        "\n",
        "\n",
        "    elif Long_Format == False and Short_Format == True:\n",
        "        prompt,short_answer = answer_query(query,temperature=0.6,long_answer = False)\n",
        "        short_answer = pretty_asnwer(prompt,short_answer)\n",
        "\n",
        "        while len(short_answer)<30: # Generate again if, last time model was disable\n",
        "          prompt,short_answer = answer_query(query,temperature=0.6,long_answer = False)\n",
        "          short_answer = pretty_asnwer(prompt,short_answer)\n",
        "\n",
        "        if Audio_Play:\n",
        "          text = 'short answer:'+ short_answer\n",
        "          audio_ouput = text_to_speech(text)\n",
        "          return ' ',short_answer , audio_ouput\n",
        "\n",
        "        else:\n",
        "          text = ' You did not chose audio play'\n",
        "          audio_ouput = text_to_speech(text)\n",
        "          return ' ', short_answer, audio_ouput\n",
        "\n",
        "\n",
        "    else:\n",
        "        text = ' Please check one checkbox at least'\n",
        "        audio_ouput = text_to_speech(text)\n",
        "        return 'Please check one checkbox at least', 'Please check one checkbox at least', audio_ouput\n",
        "\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=aks_question,\n",
        "\n",
        "    inputs=[\"text\", \"checkbox\",\"checkbox\",\"checkbox\"],\n",
        "\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Long Answer\"),\n",
        "        gr.Textbox(label=\"Short Answer\"),\n",
        "        gr.Audio(label='Long_Format_Audio_Play')\n",
        "    ]\n",
        ")\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "HpDSX0eLLEJz",
        "execution": {
          "iopub.status.busy": "2024-05-24T10:53:38.735672Z",
          "iopub.execute_input": "2024-05-24T10:53:38.736673Z",
          "iopub.status.idle": "2024-05-24T10:53:44.045896Z",
          "shell.execute_reply.started": "2024-05-24T10:53:38.736633Z",
          "shell.execute_reply": "2024-05-24T10:53:44.044474Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 15GwtRsgdPNkgQ-15WLsmZa6Dsd-hijmH\n"
      ],
      "metadata": {
        "id": "E3zBhXonyVYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ab31704-6243-4f6f-956e-c71f5f915751"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=15GwtRsgdPNkgQ-15WLsmZa6Dsd-hijmH\n",
            "From (redirected): https://drive.google.com/uc?id=15GwtRsgdPNkgQ-15WLsmZa6Dsd-hijmH&confirm=t&uuid=27bd57ba-b535-4c18-a29f-0b5ae404cb2d\n",
            "To: /content/checkpoint_epoch_1_batch_40000.pt\n",
            "100% 2.68G/2.68G [00:41<00:00, 65.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n",
        "import random\n",
        "import os\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Training parameters\n",
        "epochs = 2\n",
        "batch_size = 1\n",
        "learning_rate = 5e-5\n",
        "adam_epsilon = 1e-8\n",
        "warmup_steps = 0\n",
        "train_questions = 91772\n",
        "\n",
        "\n",
        "# Load the T5 tokenizer and model\n",
        "model_name = 't5-base'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, eps=adam_epsilon)\n",
        "total_steps = (train_questions) // batch_size * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
        "    checkpoint = torch.load(checkpoint_path,  map_location=torch.device(device))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1  # start from the next epoch\n",
        "    loss = checkpoint['loss']\n",
        "    return model, optimizer, scheduler, start_epoch, loss\n",
        "\n",
        "# Initialize variables\n",
        "start_epoch = 0  # default start epoch\n",
        "checkpoint_path = '/content/checkpoint_epoch_1_batch_40000.pt'\n",
        "\n",
        "model, optimizer, scheduler, start_epoch, loss = load_checkpoint(checkpoint_path, model, optimizer, scheduler)\n",
        "print(f\"Loaded checkpoint from epoch {start_epoch}, loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ4vJM01ZJYi",
        "outputId": "c5d5681a-eea8-4381-aea5-12d656b7d57e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from epoch 2, loss: 75456.31543746311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(question, context):\n",
        "    input_text = f\"question: {question} context: {' '.join(context[0] + context[1])}\"\n",
        "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(device)\n",
        "    outputs = model.generate(inputs, min_length=100, max_length=1024, num_beams=5, early_stopping=True).to(device)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "cntx = find_relevant_context_SBERT(question, question_embeddings, all_answers, all_scores, faiss_index, use='test')\n",
        "answer = generate_answer(question, cntx)\n",
        "print(answer)"
      ],
      "metadata": {
        "id": "6GggyjUUauU-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}